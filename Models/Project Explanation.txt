***Project Explanation: System-by-System Breakdown***

1. Project Aim
    The project uses reinforcement learning (RL) to predict the sustainability and lifespan of electronic devices. 
    It is an interactive tool where users input specific information about their device 
    (e.g., usage hours, condition, battery health, etc.), 
    and the system computes a sustainability score and an estimate of the lifespan.
    It also gives individualized recommendations for enhancing sustainability and performance for the device.
 
2. User Interaction System
    It obtains device type specific information via the prompts.
    Every kind of device type is different in their features: e.g. Laptop, Smartphone, Tablet.
    This is done to guarantee the data received to be correct with the exact reality.
 
Key Features:
    a. Condition Mapping: Translates user-defined device conditions, such as "Good" or "Fair," into numerical values for computation.
    b. Feature Inputs: Features include battery health, usage hours, or device age, depending on the type of device.
    c. Error Handling: Ensures valid inputs by checking (for example, rejecting non-numeric values for numerical inputs).
    d. Purpose: This system bridges the gap between the user and the RL model by formatting input data appropriately.

3. Reinforcement Learning Environment
    The custom environment (ElectronicsEnv) defines the RL system's observation and action spaces.

    Features:
    a. Observation Space: Encodes device features such as sustainability score, lifespan, and other device-specific metrics.
    b. Action Space: Actions include usage cut, maintenance of, or upgrade on the device
    c. Reward System: Weights: Sustainability 70 % and Lifespan 30 % for overall long-term sustainable and performance optimization
    d. Purpose: This system mirrors how various action impacts the device's sustainability as well as lifetime, thus providing training to a RL agent through optimal decision making.

4. Machine Learning Model
    The project uses DQN from the Stable-Baselines3 library to train the RL agent.

    Important Steps
    a. Device selection dictates the dimension of the observation space.
    b. Model Training : The DQN would learn the optimal actions on the given 10,000 timesteps considering exploration-exploitation trade-off.
    c. Forecasting: Upon training, the agent predicts what is the best action for a given state input by the user.
    d. Goal: The RL model captures the usage patterns on devices and makes suggestions to the user to maximize sustainability and increase lifespan.

5. Suggestions System
    The system, given the user's inputs and predictions by the RL, produces actionable recommendations for each device type.

    Examples:

    a. Smartphones: change battery when health drops below 50 and limit screen use to avoid overheating
    b. Laptops: Upgrade RAM to at least 8 GB. If the old batteries are present, replace them or improve daily usage for better energy usage efficiency.
    c. Smartwatches: limit usage above 16 hours/ day. Regularly install updated software versions to enhance sustainability.
    d. Purpose: Translates predictions into actionable insights that users can apply to improve the efficiency of their device.

6. Output Interpretation
    The final output comprises of:

    a. Predicted Sustainability Score: A number that reflects the sustainability and operational efficiency of the device.
    b. Predicted Lifespan: An estimated number of years the device will be functional.
    c. Tailored Recommendations: Actionable steps users can take to upgrade the condition of their device.
    d. Purpose: To educate users on how their device is currently and enable them to make a meaningful decision.